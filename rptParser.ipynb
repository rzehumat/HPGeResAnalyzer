{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import dateutil.parser as dparser\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_header(lines):\n",
    "    header = {}\n",
    "    datetime_columns = []\n",
    "\n",
    "    for line in lines:\n",
    "        if not line.isspace():\n",
    "            if \":\" in line:\n",
    "                new_key, new_val = [x.strip() for x in line.split(':',maxsplit=1)]\n",
    "                if new_val == \"\":\n",
    "                    continue\n",
    "                if re.match(\"\\d{1,2}\\.\\d{1,2}\\.\\d{2,4}[ \\d{1,2}\\:\\d{1,2}\\:\\d{1,2}]*\", new_val):\n",
    "                    new_val = dparser.parse(new_val, fuzzy=True)\n",
    "                    datetime_columns.append(new_key)\n",
    "\n",
    "                header[new_key] = new_val\n",
    "    \n",
    "    return header, datetime_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(lines):\n",
    "    # UGLY, could be one-liner, or?\n",
    "    head = lines[0].split()\n",
    "    params_no = len(head)\n",
    "\n",
    "    data = dict.fromkeys(head)\n",
    "\n",
    "    # UGLY, I believe there shoould be a way of doing it without for loop\n",
    "    for key in data:\n",
    "        data[key] = []\n",
    "\n",
    "    for line in lines[1:]:\n",
    "        if not \"Bkgnd\" in line:\n",
    "            if len(line.split()) == params_no:\n",
    "                # UGLY, do it in one line\n",
    "                for i in range(params_no):\n",
    "                    data[head[i]].append(line.split()[i])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_one_RPT(rpt_file):\n",
    "    my_file = open(rpt_file, 'r')\n",
    "    lines = my_file.readlines()\n",
    "\n",
    "    lookup = \"Bkgnd\"\n",
    "    for line in lines:\n",
    "        if lookup in line:\n",
    "            split_num = lines.index(line)\n",
    "            break\n",
    "\n",
    "    header_lines = lines[0:split_num]\n",
    "    data_lines = lines[split_num:-1]\n",
    "\n",
    "    header, datetime_columns = parse_header(header_lines)\n",
    "    data = parse_data(data_lines)\n",
    "\n",
    "    meta_df = pd.DataFrame(header, index=[0])\n",
    "    rpt_df = pd.DataFrame(data)\n",
    "    res = pd.concat([rpt_df, meta_df], axis=1)\n",
    "    res = res.fillna(method='ffill')\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polish_dtypes(df):\n",
    "    DATETIME_COLUMN = \"Report Generated On\"\n",
    "    UNSIGNED_COLUMNS = [\"Pk\", \"Area\", \"Bkgnd\", \"Left\", \"PW\", \"Sample Identification\"]\n",
    "    INT_COLUMNS = [\"IT\", \"Sample Type\"]\n",
    "    FLOAT_COLUMNS = [\"Energy\", \"FWHM\", \"Channel\", \"Cts/Sec\", \"%err\", \"Fit\", \"Peak Locate Threshold\"]\n",
    "    TO_DROP = [\"Sample Geometry\", \"Peak Locate Range (in channels)\", \"Sample Size\", \"Dead Time\", \"Peak Analysis Report                    26.11.2020  5\", \"Peak Analysis From Channel\", \"Peak Search Sensitivity\", \"Max Iterations\", \"Use Fixed FWHM\", \"Peak Fit Engine Name\"]\n",
    "    cols = df.columns.tolist()\n",
    "    if DATETIME_COLUMN in cols:\n",
    "        df[DATETIME_COLUMN] = pd.to_datetime(df[DATETIME_COLUMN])\n",
    "    for col in UNSIGNED_COLUMNS:\n",
    "        df[col] = pd.to_numeric(df[col], downcast = \"unsigned\")\n",
    "    for col in INT_COLUMNS:\n",
    "        df[col] = pd.to_numeric(df[col], downcast = \"integer\")\n",
    "    for col in FLOAT_COLUMNS:\n",
    "        df[col] = pd.to_numeric(df[col], downcast = \"float\")\n",
    "    for col in [\"Real Time\", \"Live Time\", \"Identification Energy Tolerance\"]:\n",
    "        if col in cols:\n",
    "            unit = str(df[col][0]).split()[-1]\n",
    "            df[col] = (df[col].str.split(\" \").str[0]).astype(float)\n",
    "            df[col] = df[col].rename(f\"{col} [{unit}]\")\n",
    "\n",
    "    if \"Dead Time\" in cols:\n",
    "        df[\"Dead Time (rel)\"] = 0.01 * (df[\"Dead Time\"].str.split(\" \").str[0]).astype(float)\n",
    "\n",
    "    df = df.drop(columns = TO_DROP, errors = \"ignore\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_RPT(folder):\n",
    "    if not os.path.isdir(folder):\n",
    "        raise Exception(f\"Folder '{folder}' not found. Consider creating it and moving RPT files there.\")\n",
    "    \n",
    "    Path(\"./parsed_reports\").mkdir(parents=True, exist_ok=True)\n",
    "    for rpt_file in glob.iglob(f\"{folder}/*.RPT\"):\n",
    "        res_df = parse_one_RPT(rpt_file)\n",
    "\n",
    "        name = (rpt_file.split('.')[-2]).split('/')[-1]\n",
    "        res_df = polish_dtypes(res_df)\n",
    "\n",
    "        res_df.to_csv(f\"parsed_reports/{name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_RPT(\"raw_reports\")"
   ]
  }
 ]
}